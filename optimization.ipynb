{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import json\n",
    "import pymupdf\n",
    "from llama_cpp import  Llama\n",
    "import re\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from optimization.src.optimizer import ChunkOptimizer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "import uuid\n",
    "from openai import OpenAI\n",
    "import re\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain import HuggingFacePipeline\n",
    "import nest_asyncio\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = Llama(model_path='C:/Users/shour/.cache/lm-studio/models/second-state/All-MiniLM-L6-v2-Embedding-GGUF/all-MiniLM-L6-v2-Q4_0.gguf', \n",
    "                    embedding=True,\n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\I'\n",
      "C:\\Users\\shour\\AppData\\Local\\Temp\\ipykernel_26780\\3795922768.py:3: SyntaxWarning: invalid escape sequence '\\I'\n",
      "  insurance_act = 'insurance-information\\Insurance Act,1938 - incorporating all amendments till 20212021-08-12.pdf'\n"
     ]
    }
   ],
   "source": [
    "documents = {}\n",
    "\n",
    "insurance_act = 'insurance-information\\Insurance Act,1938 - incorporating all amendments till 20212021-08-12.pdf'\n",
    "\n",
    "doc = pymupdf.open(insurance_act)\n",
    "\n",
    "complete_text = \"\"\n",
    "for page in doc.pages(12):\n",
    "    text = page.get_text()\n",
    "    text = text[2:]\n",
    "    paras = text.split('\\n \\n \\n1')[:-1]\n",
    "    for para in paras:\n",
    "        complete_text += para\n",
    "\n",
    "text = re.sub(r'\\d\\*{1,3}', '', complete_text)\n",
    "text = re.sub(r'\\n\\d+', '', text)\n",
    "text = re.sub(r'\\(\\d+\\)', '', text)\n",
    "text = text.replace('*', '').replace('\\n \\n', '\\n').replace('  ', ' ').replace('  ', ' ').replace('[', '').replace(']', '').replace(' \\n', '\\n').replace(' .', '.').replace('..', '.')\n",
    "text = text.strip()\n",
    "documents['insurance_act'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\shour\\AppData\\Local\\Temp\\ipykernel_26780\\578437280.py:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  policyholder_file = 'insurance-information\\Draft IRDAI(Protection of Policyholders’ Interests and Allied Matters of Insurers) Regulations, 2024.pdf'\n"
     ]
    }
   ],
   "source": [
    "policyholder_file = 'insurance-information\\Draft IRDAI(Protection of Policyholders’ Interests and Allied Matters of Insurers) Regulations, 2024.pdf'\n",
    "\n",
    "doc = pymupdf.open(policyholder_file)\n",
    "\n",
    "complete_text = \"\"\n",
    "for page in doc.pages(2):\n",
    "    text = page.get_text()\n",
    "    complete_text += text\n",
    "complete_text = complete_text[235:]\n",
    "\n",
    "text = re.sub(r'\\d+\\s*\\|\\s*P\\s*a\\s*g\\s*e', '', complete_text)\n",
    "text = re.sub(r'\\(\\d+\\)', '', text)\n",
    "text = re.sub(r'\\n\\d+', '', text)\n",
    "text = text.replace('  ', '').replace('\\n \\n', '\\n').replace('\\n\\n', '\\n').replace('. \\n', '').replace('*','').replace('__', '_')\n",
    "documents['policyholder'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\L'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\L'\n",
      "C:\\Users\\shour\\AppData\\Local\\Temp\\ipykernel_26780\\3522902228.py:1: SyntaxWarning: invalid escape sequence '\\L'\n",
      "  handbook_path = 'insurance-information\\Life Insurance Handbook (English).pdf'\n"
     ]
    }
   ],
   "source": [
    "handbook_path = 'insurance-information\\Life Insurance Handbook (English).pdf'\n",
    "\n",
    "doc = pymupdf.open(handbook_path)\n",
    "\n",
    "complete_text = \"\"\n",
    "for page in doc.pages(2):\n",
    "    text = page.get_text()\n",
    "    complete_text += text\n",
    "\n",
    "text = re.sub(r'\\n\\d+', '', complete_text)\n",
    "text = text.replace('  ', '').replace('\\n \\n', '\\n').replace('\\n\\n', '\\n').replace('. \\n', '').replace('•', '')\n",
    "documents['handbook'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLocalEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts):\n",
    "        return [np.array(embed_model.create_embedding(text)['data'][0]['embedding']) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return np.array(embed_model.create_embedding(text)['data'][0]['embedding'])\n",
    "    \n",
    "embeddings = MyLocalEmbeddings()\n",
    "semantic_splitter = SemanticChunker(embeddings)\n",
    "\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "token_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=0,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\",\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_splits  = semantic_splitter.split_text(documents['insurance_act'])\n",
    "char_splits      = char_splitter.split_text(documents['insurance_act'])\n",
    "token_splits     = token_splitter.split_text(documents['insurance_act'])\n",
    "recursive_splits = recursive_splitter.split_text(documents['insurance_act'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for method are: \n",
      "Number of splits are : 32\n",
      "Minimum length of split is : 7\n",
      "Maximum length of split is : 36823\n",
      "The average length of split is : 4776.689655172414\n",
      "Minimum length of tokens is : 8\n",
      "Maximum length of tokens is : 7605\n",
      "The average length of tokens is : 1021.2068965517242\n",
      "Statistics for method are: \n",
      "Number of splits are : 72\n",
      "Minimum length of split is : 702\n",
      "Maximum length of split is : 1999\n",
      "The average length of split is : 1917.5862068965516\n",
      "Minimum length of tokens is : 168\n",
      "Maximum length of tokens is : 586\n",
      "The average length of tokens is : 412.6034482758621\n",
      "Statistics for method are: \n",
      "Number of splits are : 16\n",
      "Minimum length of split is : 3802\n",
      "Maximum length of split is : 9480\n",
      "The average length of split is : 8645.8\n",
      "Minimum length of tokens is : 913\n",
      "Maximum length of tokens is : 1947\n",
      "The average length of tokens is : 1842.9333333333334\n",
      "Statistics for method are: \n",
      "Number of splits are : 21\n",
      "Minimum length of split is : 2049\n",
      "Maximum length of split is : 9645\n",
      "The average length of split is : 6607.952380952381\n",
      "Minimum length of tokens is : 511\n",
      "Maximum length of tokens is : 1979\n",
      "The average length of tokens is : 1410.4761904761904\n"
     ]
    }
   ],
   "source": [
    "splits = [semantic_splits, char_splits, token_splits, recursive_splits]\n",
    "\n",
    "for split in splits:\n",
    "    print(\"Statistics for method are: \")\n",
    "    split_lens = {len(x): len(embed_model.tokenize(text=x.encode('utf-8'))) for x in split}\n",
    "    print(f\"Number of splits are : {len(split)}\")\n",
    "    print(f\"Minimum length of split is : {min(split_lens.keys())}\")\n",
    "    print(f\"Maximum length of split is : {max(split_lens.keys())}\")\n",
    "    print(f\"The average length of split is : {sum(list(split_lens.keys())) / len(list(split_lens.keys()))}\")\n",
    "    print(f\"Minimum length of tokens is : {min(split_lens.values())}\")\n",
    "    print(f\"Maximum length of tokens is : {max(split_lens.values())}\")\n",
    "    print(f\"The average length of tokens is : {sum(list(split_lens.values())) / len(list(split_lens.keys()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_scores(split_embeddings):\n",
    "    for i in range(len(split_embeddings.keys())):\n",
    "        if i == len(split_embeddings.keys()) - 1:\n",
    "            score = 0\n",
    "        else:\n",
    "            score = cosine_similarity(split_embeddings[i]['Embedding'], split_embeddings[i+1]['Embedding'])[0][0]        \n",
    "        split_embeddings[i]['Score'] = score\n",
    "    return split_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_methods = ['semantic', 'char', 'token', 'recursive']\n",
    "\n",
    "split_dict = {'semantic' : {'splits' : semantic_splits},\n",
    "              'char' : {'splits' : char_splits},\n",
    "              'token' : {'splits' : token_splits},\n",
    "              'recursive' : {'splits' : recursive_splits}}\n",
    "\n",
    "for split_type in split_dict.keys():\n",
    "    split_dict[split_type]['base_embeddings'] =  {idx : {'Text' : x, 'Embedding' : np.array(embed_model.create_embedding(x)['data'][0]['embedding']).reshape(1, -1), 'can_join' : True} for idx, x in enumerate(split_dict[split_type]['splits'])}\n",
    "    split_dict[split_type]['base_embeddings'] = find_scores(split_dict[split_type]['base_embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_smaller(split_embeddings, min_chunk_size):\n",
    "    to_remove = []\n",
    "    for i in split_embeddings.keys():\n",
    "        if len(split_embeddings[i]['Text']) < min_chunk_size:\n",
    "            if((split_embeddings[i]['Score'] >= split_embeddings[max(i-1,0)]['Score']) or ((i-1) in to_remove)):\n",
    "                split_embeddings[i+1]['Text'] += split_embeddings[i]['Text']\n",
    "                split_embeddings[i+1]['Embedding'] = np.add(split_embeddings[i]['Embedding'], split_embeddings[i+1]['Embedding'])\n",
    "                to_remove.append(i)\n",
    "            else:\n",
    "                split_embeddings[i-1]['Text'] += split_embeddings[i]['Text']\n",
    "                split_embeddings[i-1]['Embedding'] = np.add(split_embeddings[i]['Embedding'], split_embeddings[i-1]['Embedding'])\n",
    "                to_remove.append(i)\n",
    "    split_embeddings = {k: v for k, v in split_embeddings.items() if k not in to_remove}\n",
    "    return split_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max(split_embeddings):\n",
    "    return max(d['Score'] for d in split_embeddings.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split(text1, text2='', min_len = 200, max_len=2000, overlap=0):\n",
    "    text = text1 + text2\n",
    "    splits = []\n",
    "    n_splits = len(text) / min_len\n",
    "    n_splits = np.random.randint(2,n_splits)\n",
    "    part_size = len(text) // n_splits\n",
    "    splits = [text[i:i+part_size+overlap] for i in range(0, len(text), part_size)]\n",
    "        \n",
    "    embeddings = []\n",
    "    for split in splits:\n",
    "        embeddings.append(np.array(embed_model.create_embedding(split)['data'][0]['embedding']).reshape(1, -1))\n",
    "    scores = []\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        score = cosine_similarity(embeddings[i], embeddings[i+1])[0][0]\n",
    "        scores.append((sum(scores) + score) / (len(scores) + 1))\n",
    "    try:\n",
    "        split_index = scores.index(min(scores))\n",
    "        if split_index != 0:\n",
    "            text1 = ' '.join(splits[:split_index])\n",
    "            text2 = ' '.join(splits[split_index:])\n",
    "        else:\n",
    "            text1 = ' '.join(splits[:split_index+1])\n",
    "            text2 = ' '.join(splits[split_index+1:])\n",
    "    except:\n",
    "        print(scores)\n",
    "        print(len(splits))\n",
    "        print(len(embeddings))\n",
    "        print(split_index)\n",
    "    response = []\n",
    "    if(len(text1) < max_len):\n",
    "        response.extend([text1])\n",
    "    elif len(text1) > max_len:\n",
    "        text1 = find_split(text1=text1, min_len=min_len, max_len=max_len, overlap=overlap)\n",
    "        response.extend(text1)\n",
    "    if(len(text2) < max_len):\n",
    "        response.extend([text2])\n",
    "    elif len(text2) > max_len:\n",
    "        text2 = find_split(text1=text2, min_len=min_len, max_len=max_len, overlap=overlap)\n",
    "        response.extend(text2)\n",
    "    return list(set(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_bigger(split_embeddings, min_len, max_len, overlap):\n",
    "    to_add = []\n",
    "    to_remove = []\n",
    "    for i in split_embeddings.keys():\n",
    "        if len(split_embeddings[i]['Text']) > max_len:\n",
    "            smaller_chunks = find_split(split_embeddings[i]['Text'], min_len=min_len, max_len=max_len, overlap=overlap)\n",
    "            to_add.extend(smaller_chunks)\n",
    "            to_remove.append(i)\n",
    "    to_add = list(set(to_add))\n",
    "    for i in range(len(to_add)):\n",
    "        text = to_add[i]\n",
    "        if len(text) > max_len:\n",
    "            raise ValueError(\"Length of new chunk is greater than maximum length\")\n",
    "        embed = np.array(embed_model.create_embedding(text)['data'][0]['embedding']).reshape(1, -1)\n",
    "        split_embeddings[i+len(split_embeddings.keys())] = {'Text' : text, 'Embedding' : embed, 'Score' : 0, 'can_join' : False}\n",
    "    split_embeddings = {k: v for k, v in split_embeddings.items() if k not in to_remove}\n",
    "    return split_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_embeddings(max_len, threshold, split_embeddings, min_len, overlap, repeat=False):\n",
    "    split_embeddings = combine_smaller(split_embeddings, min_len)\n",
    "    split_embeddings = break_bigger(split_embeddings, min_len, max_len, overlap)\n",
    "    temp_dict = {i: split_embeddings[k] for i, k in enumerate(sorted(split_embeddings.keys()))}\n",
    "    split_embeddings = temp_dict\n",
    "    split_embeddings = find_scores(split_embeddings)\n",
    "    max_score = find_max(split_embeddings)\n",
    "    counter = 1\n",
    "    while max_score > threshold:\n",
    "        to_delete = []\n",
    "        texts_add = []\n",
    "        for i in split_embeddings.keys():\n",
    "            if(split_embeddings[i]['Score'] > threshold and split_embeddings[i]['can_join'] == True):\n",
    "                if(len(split_embeddings[i]['Text'] + split_embeddings[i+1]['Text'])) < max_len:\n",
    "                    split_embeddings[i+1]['Embedding'] = np.add(split_embeddings[i]['Embedding'], split_embeddings[i+1]['Embedding'])\n",
    "                    split_embeddings[i+1]['Text']      = split_embeddings[i]['Text'] + split_embeddings[i+1]['Text']\n",
    "                    to_delete.append(i)\n",
    "                else:\n",
    "                    smaller_chunks = find_split(split_embeddings[i]['Text'], split_embeddings[i+1]['Text'], min_len=min_len, max_len=max_len, overlap=overlap)\n",
    "                    texts_add.append(smaller_chunks)\n",
    "                    to_delete.extend([i, i+1])\n",
    "        to_delete = set(to_delete)\n",
    "        num_chunks = list(split_embeddings.keys())[-1]\n",
    "        new_chunks = 1\n",
    "        for j in range(len(texts_add)):\n",
    "            for i in range(len(texts_add[j])):\n",
    "                text = texts_add[i]\n",
    "                embed = np.array(embed_model.create_embedding(text)['data'][0]['embedding']).reshape(1, -1)\n",
    "                split_embeddings[num_chunks + new_chunks] = {'Text' : text, 'Embedding' : embed, 'Score' : 0}\n",
    "                if i == len(texts_add[j]) - 1:\n",
    "                    split_embeddings[num_chunks + new_chunks]['can_join'] = True\n",
    "                else:\n",
    "                    split_embeddings[num_chunks + new_chunks]['can_join'] = False\n",
    "                new_chunks += 1\n",
    "        if(len(to_delete) == 0) and repeat:\n",
    "            return split_embeddings\n",
    "        elif len(to_delete) == 0:\n",
    "            repeat = True\n",
    "            continue\n",
    "        split_embeddings = {k: v for k, v in split_embeddings.items() if k not in to_delete}\n",
    "        final_len = len(split_embeddings.keys())\n",
    "        temp_dict = {i: split_embeddings[k] for i, k in enumerate(sorted(split_embeddings.keys()))}\n",
    "        split_embeddings = temp_dict\n",
    "        find_scores(split_embeddings)\n",
    "        max_score = find_max(split_embeddings)\n",
    "        print(f\"After {counter} iterations, the number of splits are : {final_len}. The highest similarity score is : {max_score}\")\n",
    "        counter += 1\n",
    "        repeat = False\n",
    "    return split_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1 iterations, the number of splits are : 195. The highest similarity score is : 0.7296680204798144\n"
     ]
    }
   ],
   "source": [
    "for split_type in split_dict.keys():\n",
    "    split_dict[split_type]['optimize_embeddings'] =  combine_embeddings(2000, 0.7, split_dict[split_type]['base_embeddings'], 500, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For split type semantic, the number of base chunks are : 32 and optimized chunks are : 117\n",
      "The minimum length for base is 10 and optimized is 540\n",
      "The maximum length for base is 37032 and optimized is 1988\n",
      "The average length for base is 4410.90625 and optimized is 1145.2051282051282\n",
      "For split type char, the number of base chunks are : 72 and optimized chunks are : 195\n",
      "The minimum length for base is 702 and optimized is 3\n",
      "The maximum length for base is 1999 and optimized is 1922\n",
      "The average length for base is 1925.611111111111 and optimized is 16.8\n",
      "For split type token, the number of base chunks are : 16 and optimized chunks are : 122\n",
      "The minimum length for base is 3802 and optimized is 565\n",
      "The maximum length for base is 9480 and optimized is 1991\n",
      "The average length for base is 8669.1875 and optimized is 1140.7377049180327\n",
      "For split type recursive, the number of base chunks are : 21 and optimized chunks are : 123\n",
      "The minimum length for base is 2049 and optimized is 545\n",
      "The maximum length for base is 9645 and optimized is 1973\n",
      "The average length for base is 6607.952380952381 and optimized is 1131.5447154471544\n"
     ]
    }
   ],
   "source": [
    "for split_type in split_dict.keys():\n",
    "    base_lens = [len(x['Text']) for x in split_dict[split_type]['base_embeddings'].values()]\n",
    "    optimized_lens = [len(x['Text']) for x in split_dict[split_type]['optimize_embeddings'].values()]\n",
    "    print(f\"For split type {split_type}, the number of base chunks are : {len(base_lens)} and optimized chunks are : {len(optimized_lens)}\")\n",
    "    print(f\"The minimum length for base is {min(base_lens)} and optimized is {min(optimized_lens)}\")\n",
    "    print(f\"The maximum length for base is {max(base_lens)} and optimized is {max(optimized_lens)}\")\n",
    "    print(f\"The average length for base is {sum(base_lens) / len(base_lens)} and optimized is {sum(optimized_lens) / len(optimized_lens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = QdrantClient(path=\"rag_eval\")\n",
    "\n",
    "client.create_collection(collection_name='semantic', vectors_config=VectorParams(size=384, distance=Distance.COSINE))\n",
    "client.create_collection(collection_name='char', vectors_config=VectorParams(size=384, distance=Distance.COSINE))\n",
    "client.create_collection(collection_name='token', vectors_config=VectorParams(size=384, distance=Distance.COSINE))\n",
    "client.create_collection(collection_name='recursive', vectors_config=VectorParams(size=384, distance=Distance.COSINE))\n",
    "client.create_collection(collection_name='optimized-semantic', vectors_config=VectorParams(size=384, distance=Distance.COSINE))\n",
    "client.create_collection(collection_name='optimized-char', vectors_config=VectorParams(size=384, distance=Distance.COSINE))\n",
    "client.create_collection(collection_name='optimized-token', vectors_config=VectorParams(size=384, distance=Distance.COSINE))\n",
    "client.create_collection(collection_name='optimized-recursive', vectors_config=VectorParams(size=384, distance=Distance.COSINE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['semantic', 'char', 'token', 'recursive'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection for semantic splits\n",
    "points = [\n",
    "  PointStruct(\n",
    "    id = str(uuid.uuid4()),\n",
    "    vector = chunk['Embedding'].flatten().tolist(),\n",
    "    payload = {\n",
    "      \"text\": chunk['Text']\n",
    "    }\n",
    "  )\n",
    "  for chunk in split_dict['semantic']['base_embeddings'].values()]\n",
    "\n",
    "operation_info = client.upsert(\n",
    "    collection_name='semantic',\n",
    "    wait=True,\n",
    "    points=points\n",
    ")\n",
    "\n",
    "points = [\n",
    "  PointStruct(\n",
    "    id = str(uuid.uuid4()),\n",
    "    vector = chunk['Embedding'].flatten().tolist(),\n",
    "    payload = {\n",
    "      \"text\": chunk['Text']\n",
    "    }\n",
    "  )\n",
    "  for chunk in split_dict['char']['base_embeddings'].values()]\n",
    "\n",
    "operation_info = client.upsert(\n",
    "    collection_name='char',\n",
    "    wait=True,\n",
    "    points=points\n",
    ")\n",
    "\n",
    "points = [\n",
    "  PointStruct(\n",
    "    id = str(uuid.uuid4()),\n",
    "    vector = chunk['Embedding'].flatten().tolist(),\n",
    "    payload = {\n",
    "      \"text\": chunk['Text']\n",
    "    }\n",
    "  )\n",
    "  for chunk in split_dict['token']['base_embeddings'].values()]\n",
    "\n",
    "operation_info = client.upsert(\n",
    "    collection_name='token',\n",
    "    wait=True,\n",
    "    points=points\n",
    ")\n",
    "\n",
    "# Create collection for recursive splits\n",
    "points = [\n",
    "  PointStruct(\n",
    "    id = str(uuid.uuid4()),\n",
    "    vector = chunk['Embedding'].flatten().tolist(),\n",
    "    payload = {\n",
    "      \"text\": chunk['Text']\n",
    "    }\n",
    "  )\n",
    "  for chunk in split_dict['recursive']['base_embeddings'].values()]\n",
    "\n",
    "operation_info = client.upsert(\n",
    "    collection_name='recursive',\n",
    "    wait=True,\n",
    "    points=points\n",
    ")\n",
    "\n",
    "#Create collection for optimized semantic splits\n",
    "points = [\n",
    "  PointStruct(\n",
    "    id = str(uuid.uuid4()),\n",
    "    vector = chunk['Embedding'].flatten().tolist(),\n",
    "    payload = {\n",
    "      \"text\": chunk['Text']\n",
    "    }\n",
    "  )\n",
    "  for chunk in split_dict['semantic']['optimize_embeddings'].values()]\n",
    "\n",
    "operation_info = client.upsert(\n",
    "    collection_name='optimized-semantic',\n",
    "    wait=True,\n",
    "    points=points\n",
    ")\n",
    "\n",
    "#Create collection for optimized char splits\n",
    "points = [\n",
    "  PointStruct(\n",
    "    id = str(uuid.uuid4()),\n",
    "    vector = chunk['Embedding'].flatten().tolist(),\n",
    "    payload = {\n",
    "      \"text\": chunk['Text']\n",
    "    }\n",
    "  )\n",
    "  for chunk in split_dict['char']['optimize_embeddings'].values()]\n",
    "\n",
    "operation_info = client.upsert(\n",
    "    collection_name='optimized-char',\n",
    "    wait=True,\n",
    "    points=points\n",
    ")\n",
    "\n",
    "#Create collection for optimized token splits\n",
    "points = [\n",
    "  PointStruct(\n",
    "    id = str(uuid.uuid4()),\n",
    "    vector = chunk['Embedding'].flatten().tolist(),\n",
    "    payload = {\n",
    "      \"text\": chunk['Text']\n",
    "    }\n",
    "  )\n",
    "  for chunk in split_dict['token']['optimize_embeddings'].values()]\n",
    "\n",
    "operation_info = client.upsert(\n",
    "    collection_name='optimized-token',\n",
    "    wait=True,\n",
    "    points=points\n",
    ")\n",
    "\n",
    "#Create collection for optimized recursive splits\n",
    "points = [\n",
    "  PointStruct(\n",
    "    id = str(uuid.uuid4()),\n",
    "    vector = chunk['Embedding'].flatten().tolist(),\n",
    "    payload = {\n",
    "      \"text\": chunk['Text']\n",
    "    }\n",
    "  )\n",
    "  for chunk in split_dict['recursive']['optimize_embeddings'].values()]\n",
    "\n",
    "operation_info = client.upsert(\n",
    "    collection_name='optimized-recursive',\n",
    "    wait=True,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insurance-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
