{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymupdf\n",
    "import pymupdf4llm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from time import sleep\n",
    "import re\n",
    "from llama_cpp import Llama\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from optimization.src.optimizer import ChunkOptimizer\n",
    "from Evaluation.src.prompts import *\n",
    "from Evaluation.src.data_generator import DataGenerator\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = Llama(model_path='C:/Users/shour/.cache/lm-studio/models/second-state/All-MiniLM-L6-v2-Embedding-GGUF/all-MiniLM-L6-v2-Q4_0.gguf', \n",
    "                    embedding=True,\n",
    "                    verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\I'\n",
      "C:\\Users\\shour\\AppData\\Local\\Temp\\ipykernel_32616\\4116244658.py:3: SyntaxWarning: invalid escape sequence '\\I'\n",
      "  insurance_act = 'insurance-information\\Insurance Act,1938 - incorporating all amendments till 20212021-08-12.pdf'\n"
     ]
    }
   ],
   "source": [
    "documents = {}\n",
    "\n",
    "insurance_act = 'insurance-information\\Insurance Act,1938 - incorporating all amendments till 20212021-08-12.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pymupdf.open(insurance_act)\n",
    "\n",
    "complete_text = \"\"\n",
    "for page in doc.pages(12):\n",
    "    text = page.get_text()\n",
    "    text = text[2:]\n",
    "    paras = text.split('\\n \\n \\n1')[:-1]\n",
    "    for para in paras:\n",
    "        complete_text += para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'\\d\\*{1,3}', '', complete_text)\n",
    "text = re.sub(r'\\n\\d+', '', text)\n",
    "text = re.sub(r'\\(\\d+\\)', '', text)\n",
    "text = text.replace('*', '').replace('\\n \\n', '\\n').replace('  ', ' ').replace('  ', ' ').replace('[', '').replace(']', '').replace(' \\n', '\\n').replace(' .', '.').replace('..', '.')\n",
    "text = text.strip()\n",
    "documents['insurance_act'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\shour\\AppData\\Local\\Temp\\ipykernel_32616\\927278237.py:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  policyholder_file = 'insurance-information\\Draft IRDAI(Protection of Policyholders’ Interests and Allied Matters of Insurers) Regulations, 2024.pdf'\n"
     ]
    }
   ],
   "source": [
    "policyholder_file = 'insurance-information\\Draft IRDAI(Protection of Policyholders’ Interests and Allied Matters of Insurers) Regulations, 2024.pdf'\n",
    "\n",
    "doc = pymupdf.open(policyholder_file)\n",
    "\n",
    "complete_text = \"\"\n",
    "for page in doc.pages(2):\n",
    "    text = page.get_text()\n",
    "    complete_text += text\n",
    "complete_text = complete_text[235:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'\\d+\\s*\\|\\s*P\\s*a\\s*g\\s*e', '', complete_text)\n",
    "text = re.sub(r'\\(\\d+\\)', '', text)\n",
    "text = re.sub(r'\\n\\d+', '', text)\n",
    "text = text.replace('  ', '').replace('\\n \\n', '\\n').replace('\\n\\n', '\\n').replace('. \\n', '').replace('*','').replace('__', '_')\n",
    "documents['policyholder'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\L'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\L'\n",
      "C:\\Users\\shour\\AppData\\Local\\Temp\\ipykernel_32616\\1305224189.py:1: SyntaxWarning: invalid escape sequence '\\L'\n",
      "  handbook_path = 'insurance-information\\Life Insurance Handbook (English).pdf'\n"
     ]
    }
   ],
   "source": [
    "handbook_path = 'insurance-information\\Life Insurance Handbook (English).pdf'\n",
    "\n",
    "doc = pymupdf.open(handbook_path)\n",
    "\n",
    "complete_text = \"\"\n",
    "for page in doc.pages(2):\n",
    "    text = page.get_text()\n",
    "    complete_text += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'\\n\\d+', '', complete_text)\n",
    "text = text.replace('  ', '').replace('\\n \\n', '\\n').replace('\\n\\n', '\\n').replace('. \\n', '').replace('•', '')\n",
    "documents['handbook'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "token_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    model_name=\"gpt-4\",\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=0,\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_text(documents['insurance_act'])\n",
    "split_lens = {len(x): len(embed_model.tokenize(text=x.encode('utf-8'))) for x in splits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of splits are : 121\n",
      "Minimum length of split is : 262\n",
      "Maximum length of split is : 1200\n",
      "The average length of split is : 1139.4931506849316\n",
      "Minimum length of tokens is : 70\n",
      "Maximum length of tokens is : 352\n",
      "The average length of tokens is : 246.05479452054794\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of splits are : {len(splits)}\")\n",
    "print(f\"Minimum length of split is : {min(split_lens.keys())}\")\n",
    "print(f\"Maximum length of split is : {max(split_lens.keys())}\")\n",
    "print(f\"The average length of split is : {sum(list(split_lens.keys())) / len(list(split_lens.keys()))}\")\n",
    "print(f\"Minimum length of tokens is : {min(split_lens.values())}\")\n",
    "print(f\"Maximum length of tokens is : {max(split_lens.values())}\")\n",
    "print(f\"The average length of tokens is : {sum(list(split_lens.values())) / len(list(split_lens.keys()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=np.arange(len(split_lens.keys())), y=split_lens.keys())\n",
    "plt.figure(figsize=(12, 14))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_embeddings = {idx : {'Text' : x, 'Embedding' : np.array(embed_model.create_embedding(x)['data'][0]['embedding']).reshape(1, -1)} for idx, x in enumerate(splits)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_scores(split_embeddings):\n",
    "    similarity_scores = []\n",
    "    for i in range(len(split_embeddings.keys())):\n",
    "        if i == len(split_embeddings.keys()) - 1:\n",
    "            score = 0\n",
    "        else:\n",
    "            score = cosine_similarity(split_embeddings[i]['Embedding'], split_embeddings[i+1]['Embedding'])[0][0]\n",
    "        similarity_scores.append(score)\n",
    "        \n",
    "        split_embeddings[i]['Score'] = score\n",
    "    return similarity_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = find_scores(split_embeddings)\n",
    "plt.plot(similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile = 90\n",
    "print(f\"The maximum score is : {max(similarity_scores)}\")\n",
    "print(f\"The minimum score is : {min(similarity_scores)}\")\n",
    "print(f\"The {percentile} is : {np.percentile(similarity_scores, percentile)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = ChunkOptimizer(embed_model)\n",
    "split_embeddings = optimizer.optimize_chunks(splits, 1200, 200, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_model = 'meta-llama-3.1-8b-instruct'\n",
    "discriminator_model = 'meta-llama-3.1-8b-instruct'\n",
    "base_url = \"http://192.168.84.106:1234/v1\"\n",
    "\n",
    "generator = DataGenerator(generator_model_id=generator_model, discriminator_model_id=discriminator_model, base_url=base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator.context_preprocess(splits, 'base-chunks.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'base-chunks.json'  # Replace with the actual file path\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_rich_splits = [x['text'] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator.generate_qa(context_rich_splits, 'qac.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'qac.json' \n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    qac = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qac[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.qac_evaluator(qac, 'question_eval.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max(split_embeddings):\n",
    "    return max(d['Score'] for d in split_embeddings.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_max(split_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split(text1, text2='', chars=['.'], min_len = 200, max_len=2000):\n",
    "    text = text1 + text2\n",
    "    splits = []\n",
    "    i = 0\n",
    "    while((len(splits) < 2) and (i < len(chars))):\n",
    "        token_splitter = CharacterTextSplitter(\n",
    "        separator=chars[i],\n",
    "        chunk_size=(min_len * 3),\n",
    "        chunk_overlap=0,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "        )\n",
    "        splits = token_splitter.split_text(text)\n",
    "        i += 1\n",
    "    if len(splits) < 2:\n",
    "        n_splits = len(text) / min_len\n",
    "        n_splits = np.random.randint(2,n_splits)\n",
    "        part_size = len(text) // n_splits\n",
    "        splits = [text[i:i+part_size] for i in range(0, len(text), part_size)]\n",
    "        \n",
    "    embeddings = []\n",
    "    for split in splits:\n",
    "        embeddings.append(np.array(embed_model.create_embedding(split)['data'][0]['embedding']).reshape(1, -1))\n",
    "    scores = []\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        score = cosine_similarity(embeddings[i], embeddings[i+1])[0][0]\n",
    "        scores.append((sum(scores) + score) / (len(scores) + 1))\n",
    "    try:\n",
    "        split_index = scores.index(min(scores))\n",
    "        if split_index != 0:\n",
    "            text1 = ' '.join(splits[:split_index])\n",
    "            text2 = ' '.join(splits[split_index:])\n",
    "        else:\n",
    "            text1 = ' '.join(splits[:split_index+1])\n",
    "            text2 = ' '.join(splits[split_index+1:])\n",
    "    except:\n",
    "        print(scores)\n",
    "        print(len(splits))\n",
    "        print(len(embeddings))\n",
    "        print(split_index)\n",
    "    response = []\n",
    "    if((len(text1) > min_len) and (len(text1) < max_len)):\n",
    "        response.extend([text1])\n",
    "    elif len(text1) > max_len:\n",
    "        print(\"Entering nested splitter\", len(text1))\n",
    "        text1 = find_split(text1, chars=chars)\n",
    "        response.extend(text1)\n",
    "    if((len(text2) > min_len) and (len(text2) < max_len)):\n",
    "        response.extend([text2])\n",
    "    elif len(text2) > max_len:\n",
    "        print(\"Entering nested splitter\", len(text2))\n",
    "        text2 = find_split(text2, chars=chars)\n",
    "        response.extend(text2)\n",
    "    return list(set(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_embeddings(max_len, threshold, split_embeddings, min_len, repeat=False):\n",
    "    max_score = find_max(split_embeddings)\n",
    "    counter = 1\n",
    "    while max_score > threshold:\n",
    "        to_delete = []\n",
    "        texts_add = []\n",
    "        for i in split_embeddings.keys():\n",
    "            if split_embeddings[i]['Score'] > threshold:\n",
    "                if len(split_embeddings[i]['Text'] + split_embeddings[i+1]['Text']) < max_len:\n",
    "                    split_embeddings[i+1]['Embedding'] = np.add(split_embeddings[i]['Embedding'], split_embeddings[i+1]['Embedding'])\n",
    "                    split_embeddings[i+1]['Text']      = split_embeddings[i]['Text'] + ' ' + split_embeddings[i+1]['Text']\n",
    "                    to_delete.append(i)\n",
    "                else:\n",
    "                    smaller_chunks = find_split(split_embeddings[i]['Text'], split_embeddings[i+1]['Text'], ['.', ';', ','])\n",
    "                    smaller_chunks = [x for x in smaller_chunks if len(x) > min_len]\n",
    "                    texts_add.extend(smaller_chunks)\n",
    "                    to_delete.extend([i, i+1])\n",
    "        to_delete = set(to_delete)\n",
    "        for i in range(len(texts_add)):\n",
    "            text = texts_add[i]\n",
    "            if len(text) > max_len:\n",
    "                raise ValueError(\"Length of new chunk is greater than maximum length\")\n",
    "            embed = np.array(embed_model.create_embedding(text)['data'][0]['embedding']).reshape(1, -1)\n",
    "            split_embeddings[i+len(split_embeddings.keys())] = {'Text' : text, 'Embedding' : embed, 'Score' : 0}\n",
    "        if(len(to_delete) == 0) and repeat:\n",
    "            return split_embeddings\n",
    "        elif len(to_delete) == 0:\n",
    "            repeat = True\n",
    "        split_embeddings = {k: v for k, v in split_embeddings.items() if k not in to_delete}\n",
    "        final_len = len(split_embeddings.keys())\n",
    "        temp_dict = {i: split_embeddings[k] for i, k in enumerate(sorted(split_embeddings.keys()))}\n",
    "        split_embeddings = temp_dict\n",
    "        split_embeddings = temp_dict\n",
    "        find_scores(split_embeddings)\n",
    "        max_score = find_max(split_embeddings)\n",
    "        print(f\"After {counter} iterations, the number of splits are : {final_len}. The highest similarity score is : {max_score}\")\n",
    "        counter += 1\n",
    "        repeat = False\n",
    "    return split_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_embeddings = combine_embeddings(2000, 0.75, split_embeddings, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [x['Text'] for x in split_embeddings.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(x['Text']) for x in split_embeddings.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The length of shortest chunk is : {min(lens)}\")\n",
    "print(f\"The length of the longest chunk is : {max(lens)}\")\n",
    "print(f\"The average length of chunks is : {sum(lens) / len(lens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits[lens.index(min(lens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(split_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = find_scores(split_embeddings)\n",
    "plt.plot(similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile = 90\n",
    "print(f\"The maximum score is : {max(similarity_scores)}\")\n",
    "print(f\"The minimum score is : {min(similarity_scores)}\")\n",
    "print(f\"The {percentile} is : {np.percentile(similarity_scores, percentile)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = documents['policyholder_file'].split('\\n')\n",
    "splits = [x.strip() for x in splits if ((len(x) > 10) and (len(embed_model.tokenize(text=x.encode('utf-8'))) > 5))]\n",
    "split_lens = {len(x) : len(embed_model.tokenize(text=x.encode('utf-8'))) for x in splits}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of splits are : {len(splits)}\")\n",
    "print(f\"Minimum length of split is : {min(split_lens.keys())}\")\n",
    "print(f\"Maximum length of split is : {max(split_lens.keys())}\")\n",
    "print(f\"Minimum length of tokens is : {min(split_lens.values())}\")\n",
    "print(f\"Maximum length of tokens is : {max(split_lens.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insurance-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
